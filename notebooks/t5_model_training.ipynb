{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical NLQ to SQL: T5 Model Training\n",
    "\n",
    "This notebook implements the training pipeline for a T5 model that converts natural language questions (NLQ) about clinical data to SQL queries. The model is trained on the Synthea dataset schema.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Environment Setup**: Install dependencies and configure the environment\n",
    "2. **Data Generation**: Create training data of NLQ-SQL pairs\n",
    "3. **Data Preprocessing**: Prepare the data for T5 model training\n",
    "4. **Model Training**: Fine-tune a T5 model on the prepared data\n",
    "5. **Model Evaluation**: Evaluate the model's performance\n",
    "6. **Model Saving**: Save the trained model for later use\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch pandas scikit-learn sqlparse nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import sqlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EvalPrediction\n",
    ")\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving model and data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories for saving model and data\n",
    "BASE_DIR = '/content/drive/MyDrive/clinical_nlq_to_sql'\n",
    "MODEL_DIR = f'{BASE_DIR}/models/fine_tuned_t5_synthea'\n",
    "DATA_DIR = f'{BASE_DIR}/data'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Model will be saved to: {MODEL_DIR}\")\n",
    "print(f\"Data will be saved to: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected. Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation & Loading\n",
    "\n",
    "In this section, we'll generate or load diverse clinical questions and their corresponding SQL queries for the Synthea database schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Synthea database schema (simplified for notebook)\n",
    "SCHEMA_INFO = \"\"\"\n",
    "-- Clinical NLQ AI Assistant Database Schema (Simplified)\n",
    "CREATE TABLE clinical_data.patients (\n",
    "    id UUID PRIMARY KEY,\n",
    "    birth_date DATE NOT NULL,\n",
    "    death_date DATE,\n",
    "    gender VARCHAR(1) NOT NULL,\n",
    "    first_name VARCHAR(50),\n",
    "    last_name VARCHAR(50),\n",
    "    race VARCHAR(50),\n",
    "    ethnicity VARCHAR(50),\n",
    "    city VARCHAR(100),\n",
    "    state VARCHAR(50),\n",
    "    healthcare_expenses DECIMAL(12, 2)\n",
    ");\n",
    "\n",
    "CREATE TABLE clinical_data.conditions (\n",
    "    id UUID PRIMARY KEY,\n",
    "    start_date DATE NOT NULL,\n",
    "    stop_date DATE,\n",
    "    patient_id UUID NOT NULL REFERENCES clinical_data.patients(id),\n",
    "    description TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE clinical_data.medications (\n",
    "    id UUID PRIMARY KEY,\n",
    "    start_date DATE NOT NULL,\n",
    "    stop_date DATE,\n",
    "    patient_id UUID NOT NULL REFERENCES clinical_data.patients(id),\n",
    "    description TEXT NOT NULL,\n",
    "    reason_description TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Save schema to a file\n",
    "with open(f\"{DATA_DIR}/synthea_schema_simplified.sql\", \"w\") as f:\n",
    "    f.write(SCHEMA_INFO)\n",
    "\n",
    "print(f\"Schema saved to {DATA_DIR}/synthea_schema_simplified.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample NLQ-SQL pairs\n",
    "SAMPLE_QUERIES = [\n",
    "    {\n",
    "        \"nlq\": \"How many patients do we have?\",\n",
    "        \"sql\": \"SELECT COUNT(*) as patient_count FROM clinical_data.patients\",\n",
    "        \"category\": \"basic_count\"\n",
    "    },\n",
    "    {\n",
    "        \"nlq\": \"Show me all female patients\",\n",
    "        \"sql\": \"SELECT first_name, last_name, birth_date FROM clinical_data.patients WHERE gender = 'F'\",\n",
    "        \"category\": \"basic_filter\"\n",
    "    },\n",
    "    {\n",
    "        \"nlq\": \"What are the most common conditions?\",\n",
    "        \"sql\": \"SELECT description, COUNT(*) as frequency FROM clinical_data.conditions GROUP BY description ORDER BY frequency DESC LIMIT 10\",\n",
    "        \"category\": \"aggregation\"\n",
    "    },\n",
    "    {\n",
    "        \"nlq\": \"Show patients with diabetes\",\n",
    "        \"sql\": \"SELECT DISTINCT p.first_name, p.last_name, p.birth_date FROM clinical_data.patients p JOIN clinical_data.conditions c ON p.id = c.patient_id WHERE c.description ILIKE '%diabetes%'\",\n",
    "        \"category\": \"join_filter\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate more diverse NLQ-SQL pairs\n",
    "def generate_more_queries():\n",
    "    \"\"\"Generate additional NLQ-SQL pairs based on patterns\"\"\"\n",
    "    additional_queries = [\n",
    "        {\n",
    "            \"nlq\": \"Count the number of male patients\",\n",
    "            \"sql\": \"SELECT COUNT(*) as male_patient_count FROM clinical_data.patients WHERE gender = 'M'\",\n",
    "            \"category\": \"basic_filter\"\n",
    "        },\n",
    "        {\n",
    "            \"nlq\": \"How many patients are from California?\",\n",
    "            \"sql\": \"SELECT COUNT(*) as ca_patient_count FROM clinical_data.patients WHERE state = 'CA'\",\n",
    "            \"category\": \"basic_filter\"\n",
    "        },\n",
    "        {\n",
    "            \"nlq\": \"What is the average age of our patients?\",\n",
    "            \"sql\": \"SELECT AVG(EXTRACT(YEAR FROM AGE(CURRENT_DATE, birth_date))) as average_age FROM clinical_data.patients\",\n",
    "            \"category\": \"aggregation\"\n",
    "        }\n",
    "    ]\n",
    "    return additional_queries\n",
    "\n",
    "# Generate additional queries\n",
    "additional_queries = generate_more_queries()\n",
    "all_queries = SAMPLE_QUERIES + additional_queries\n",
    "\n",
    "print(f\"Total number of query pairs: {len(all_queries)}\")\n",
    "\n",
    "# Save queries to a file\n",
    "with open(f\"{DATA_DIR}/nlq_sql_pairs.json\", \"w\") as f:\n",
    "    json.dump(all_queries, f, indent=2)\n",
    "\n",
    "print(f\"Query pairs saved to {DATA_DIR}/nlq_sql_pairs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate more training examples through variations\n",
    "def generate_variations(queries, num_variations=5):\n",
    "    \"\"\"Generate variations of existing queries to expand the dataset\"\"\"\n",
    "    variations = []\n",
    "    \n",
    "    # Simple variations for demonstration\n",
    "    variations_templates = {\n",
    "        \"How many\": [\"Count the number of\", \"What is the total number of\", \"How many total\"],\n",
    "        \"Show me\": [\"Display\", \"List\", \"I want to see\", \"Can you show\"],\n",
    "        \"What are\": [\"Tell me about\", \"I need to know\", \"Could you list\", \"What's\"]\n",
    "    }\n",
    "    \n",
    "    for query in queries:\n",
    "        nlq = query[\"nlq\"]\n",
    "        sql = query[\"sql\"]\n",
    "        category = query[\"category\"]\n",
    "        \n",
    "        # Add the original query\n",
    "        variations.append(query)\n",
    "        \n",
    "        # Generate variations\n",
    "        for template, replacements in variations_templates.items():\n",
    "            if nlq.startswith(template):\n",
    "                for replacement in replacements:\n",
    "                    new_nlq = nlq.replace(template, replacement, 1)\n",
    "                    variations.append({\n",
    "                        \"nlq\": new_nlq,\n",
    "                        \"sql\": sql,\n",
    "                        \"category\": category\n",
    "                    })\n",
    "    \n",
    "    return variations\n",
    "\n",
    "# Generate variations\n",
    "expanded_queries = generate_variations(all_queries)\n",
    "print(f\"Expanded to {len(expanded_queries)} query pairs with variations\")\n",
    "\n",
    "# Save expanded queries\n",
    "with open(f\"{DATA_DIR}/nlq_sql_pairs_expanded.json\", \"w\") as f:\n",
    "    json.dump(expanded_queries, f, indent=2)\n",
    "\n",
    "print(f\"Expanded query pairs saved to {DATA_DIR}/nlq_sql_pairs_expanded.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing for T5\n",
    "\n",
    "Now we'll preprocess the data for the T5 model, including tokenization and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the expanded query pairs\n",
    "with open(f\"{DATA_DIR}/nlq_sql_pairs_expanded.json\", \"r\") as f:\n",
    "    expanded_queries = json.load(f)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(expanded_queries)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the input and target for T5\n",
    "def format_for_t5(row, schema_info):\n",
    "    \"\"\"Format the input and target for T5 model\"\"\"\n",
    "    # Input: question + schema context\n",
    "    input_text = f\"question: {row['nlq']} context: {schema_info}\"\n",
    "    \n",
    "    # Target: SQL query\n",
    "    target_text = row['sql']\n",
    "    \n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"target\": target_text,\n",
    "        \"category\": row['category']\n",
    "    }\n",
    "\n",
    "# Apply formatting\n",
    "formatted_data = [format_for_t5(row, SCHEMA_INFO) for _, row in df.iterrows()]\n",
    "formatted_df = pd.DataFrame(formatted_data)\n",
    "print(f\"Formatted dataset shape: {formatted_df.shape}\")\n",
    "formatted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train_df, temp_df = train_test_split(formatted_df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train set: {train_df.shape[0]} examples\")\n",
    "print(f\"Validation set: {val_df.shape[0]} examples\")\n",
    "print(f\"Test set: {test_df.shape[0]} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Set maximum input and output lengths\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Tokenize the inputs and targets\"\"\"\n",
    "    inputs = examples[\"input\"]\n",
    "    targets = examples[\"target\"]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Replace padding token id with -100 so it's ignored in loss calculation\n",
    "    labels = labels[\"input_ids\"]\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"input\", \"target\", \"category\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Configuration & Fine-tuning\n",
    "\n",
    "Now we'll configure and fine-tune the T5 model on our preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "print(f\"Model loaded: t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=f\"{MODEL_DIR}/logs\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute BLEU score and exact match accuracy\"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 with pad token id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_scores = []\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    \n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label.split()\n",
    "        bleu_score = sentence_bleu([label_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    \n",
    "    # Compute exact match accuracy\n",
    "    exact_matches = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if pred.strip() == label.strip())\n",
    "    exact_match_accuracy = exact_matches / len(decoded_preds)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": np.mean(bleu_scores),\n",
    "        \"exact_match\": exact_match_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Let's evaluate the fine-tuned model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(f\"Test results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate SQL from natural language\n",
    "def generate_sql(question, schema_info=SCHEMA_INFO):\n",
    "    \"\"\"Generate SQL query from natural language question\"\"\"\n",
    "    # Format input\n",
    "    input_text = f\"question: {question} context: {schema_info}\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Format SQL\n",
    "    formatted_sql = sqlparse.format(generated_sql, reindent=True, keyword_case='upper')\n",
    "    \n",
    "    return formatted_sql\n",
    "\n",
    "# Test with some examples\n",
    "test_questions = [\n",
    "    \"How many patients do we have?\",\n",
    "    \"Show me all female patients\",\n",
    "    \"What are the most common conditions?\",\n",
    "    \"Find patients with diabetes\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    sql = generate_sql(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Generated SQL: {sql}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Saving\n",
    "\n",
    "Finally, let's save the fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model_save_path = f\"{MODEL_DIR}/final_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model configuration and metadata\n",
    "model_metadata = {\n",
    "    \"model_name\": \"t5-small-synthea-nlq-to-sql\",\n",
    "    \"base_model\": \"t5-small\",\n",
    "    \"training_date\": pd.Timestamp.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"dataset_size\": len(expanded_queries),\n",
    "    \"train_size\": len(train_df),\n",
    "    \"val_size\": len(val_df),\n",
    "    \"test_size\": len(test_df),\n",
    "    \"test_metrics\": test_results,\n",
    "    \"max_input_length\": MAX_INPUT_LENGTH,\n",
    "    \"max_target_length\": MAX_TARGET_LENGTH,\n",
    "    \"training_args\": training_args.to_dict()\n",
    "}\n",
    "\n",
    "with open(f\"{model_save_path}/model_metadata.json\", \"w\") as f:\n",
    "    json.dump(model_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Model metadata saved to {model_save_path}/model_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Set up the